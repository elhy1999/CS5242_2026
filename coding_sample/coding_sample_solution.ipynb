{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46bcc520-6328-4d85-acae-499775e62f2c",
   "metadata": {
    "id": "46bcc520-6328-4d85-acae-499775e62f2c"
   },
   "source": [
    "## CS5242 Neural Networks and Deep Learning\n",
    "\n",
    "## Coding sample - Solution\n",
    "\n",
    "*Instructions* <br>\n",
    "Name: Please, add your name here : e.g. JOHN SMITH<br>\n",
    "Answers: Please write your answers directly in this notebook by completing the code sections marked with  \n",
    "`# YOUR CODE STARTS HERE`  \n",
    "`# YOUR CODE` (it can span one or multiple lines)  \n",
    "`# YOUR CODE ENDS HERE`. <br>\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SeVmtkbB2p5I",
   "metadata": {
    "id": "SeVmtkbB2p5I"
   },
   "source": [
    "## Exercise : Implement MLP with sigmoid activation.\n",
    "\n",
    "Implement a simple two-layer multi-layer perceptron (MLP) in PyTorch without using the pre-built modules `torch.nn.Linear()` or `torch.nn.Sigmoid()`.\n",
    "\n",
    "For the first hidden linear layer, use a weight tensor W1 with shape (in_features, hidden_size) and a bias tensor b1 with shape (hidden_size,).\n",
    "\n",
    "For the output linear layer, employ a weight tensor W2 with shape (hidden_size, out_features) and a bias tensor b2 with shape (out_features,).\n",
    "\n",
    "The activation after the hidden linear layer is the sigmoid function defined as: $\\textrm{sigmoid}(x) = 1 / (1 + \\exp(-x))$.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "`torch.exp(x):` Returns a new tensor with the exponential of the elements of the input tensor (shape unchanged).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "CYcwVLEr2ref",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1740223456939,
     "user": {
      "displayName": "刘子铭",
      "userId": "07876562238380223127"
     },
     "user_tz": -480
    },
    "id": "CYcwVLEr2ref",
    "outputId": "e1022379-0418-40bf-9c4c-e37747983532"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 26-02-09--14-43-40\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "class MyMLP:\n",
    "    \"\"\"\n",
    "    A simple MLP with one hidden layer using manual matrix multiplication\n",
    "    and manual sigmoid, without torch.nn.Linear or torch.sigmoid.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        # Weights and biases are randomly initialized with :\n",
    "        #  1) Hidden layer: (input_dim  -> hidden_dim)\n",
    "        #  2) Output layer: (hidden_dim -> output_dim)\n",
    "        self.W1 = torch.randn(input_dim, hidden_dim, requires_grad=False)\n",
    "        self.b1 = torch.randn(hidden_dim, requires_grad=False)\n",
    "        self.W2 = torch.randn(hidden_dim, output_dim, requires_grad=False)\n",
    "        self.b2 = torch.randn(output_dim, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through:\n",
    "            hidden = sigmoid(W1 * x + b1)\n",
    "            output = W2 * hidden + b2\n",
    "        \"\"\"\n",
    "        ##########################\n",
    "        # YOUR CODE STARTS HERE\n",
    "        hidden = torch.matmul(x, self.W1) + self.b1\n",
    "        hidden = 1.0 / (1.0 + torch.exp(-hidden))  # Implemented sigmoid\n",
    "        out = torch.matmul(hidden, self.W2) + self.b2\n",
    "        # YOUR CODE ENDS HERE\n",
    "        ##########################\n",
    "        return out\n",
    "\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4650d31a-5f5c-4619-ac29-cd39eedad92a",
   "metadata": {
    "id": "4650d31a-5f5c-4619-ac29-cd39eedad92a",
    "outputId": "0eea38d2-1c71-4f70-9c56-163fce83263a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well Done!\n"
     ]
    }
   ],
   "source": [
    "class ReferenceMLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Reference MLP using official PyTorch modules for comparison.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, output_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.sigmoid(self.fc1(x))\n",
    "        return self.fc2(hidden)\n",
    "\n",
    "# We set the same dimensions\n",
    "input_dim, hidden_dim, output_dim = 5, 10, 2\n",
    "\n",
    "# Create random input data\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(4, input_dim)\n",
    "\n",
    "# Create our MLP\n",
    "my_mlp = MyMLP(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Create the reference MLP\n",
    "ref_mlp = ReferenceMLP(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Copy parameters from the reference MLP to our MLP for a fair comparison\n",
    "with torch.no_grad():\n",
    "    my_mlp.W1[:] = ref_mlp.fc1.weight.data.T\n",
    "    my_mlp.b1[:] = ref_mlp.fc1.bias.data\n",
    "    my_mlp.W2[:] = ref_mlp.fc2.weight.data.T\n",
    "    my_mlp.b2[:] = ref_mlp.fc2.bias.data\n",
    "\n",
    "# Get outputs\n",
    "my_output = my_mlp.forward(x)\n",
    "ref_output = ref_mlp(x)\n",
    "\n",
    "# Compare\n",
    "diff = (my_output - ref_output).abs().max().item()\n",
    "# print(\"Difference:\", diff)\n",
    "\n",
    "# Check if the difference is very small\n",
    "if diff < 1e-7:\n",
    "    print(\"Well Done!\")\n",
    "else:\n",
    "    print(\"Try again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa60a5a-1aad-4891-8fc3-cf6b8244a3bb",
   "metadata": {
    "id": "ffa60a5a-1aad-4891-8fc3-cf6b8244a3bb"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6dfadd-e1cc-4578-abd7-5945bfa3eec2",
   "metadata": {
    "id": "ce6dfadd-e1cc-4578-abd7-5945bfa3eec2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
