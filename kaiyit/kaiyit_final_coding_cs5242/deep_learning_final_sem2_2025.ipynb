{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46bcc520-6328-4d85-acae-499775e62f2c",
   "metadata": {
    "id": "46bcc520-6328-4d85-acae-499775e62f2c"
   },
   "source": [
    "## CS5242 Neural Networks and Deep Learning\n",
    "## Sem 2 2024/25\n",
    "### Lecturer: Xavier Bresson\n",
    "### Teaching Assistants: Liu Nian, Liu Ziming, Liu Xiaokang, Yan Zehong, Chew KinWhye\n",
    "\n",
    "## Final exam, coding test\n",
    "Date: April 14 2025 <br>\n",
    "Time: 6:45pm-8:15pm (90min) <br>\n",
    "\n",
    "*Instructions* <br>\n",
    "Name: Please, add your name here : KOH KAI YIT<br>\n",
    "Answers: Please write your answers directly in this notebook by completing the code sections marked with  \n",
    "`# YOUR CODE STARTS HERE`  \n",
    "`# YOUR CODE` (it can span one or multiple lines)  \n",
    "`# YOUR CODE ENDS HERE`. <br>\n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LYZhVNCs2kZV",
   "metadata": {
    "id": "LYZhVNCs2kZV"
   },
   "source": [
    "## Exercise 1 : Implement a New Recurrent Neural Network\n",
    "\n",
    "In this exercise, you will design and train a new vanilla RNN for next-word prediction using a subset of the Penn Treebank (PTB) dataset.\n",
    "\n",
    "Unlike the standard RNN cell, this model uses a multiplicative interaction between the input and the recurrent transformation, followed by a ReLU nonlinearity:\n",
    "$$\n",
    "h_t = \\textrm{ReLU}\\big( (W_R h_{t-1} + b_R) \\odot (W_V g_t + b_V) \\big)\n",
    "$$\n",
    "where:\n",
    "- $g_t$ is the input word embedding at time step $t$,\n",
    "- $h_{t-1}$ is the hidden state from the previous step $t-1$ (i.e. the memory of past tokens),\n",
    "- $W_R, W_V$ are weight matrices (learnable),\n",
    "- $b_R, b_V$ are biases,\n",
    "- $\\odot$ is element-wise matrix multiplication,\n",
    "- ReLU is the standard rectified linear unit activation.\n",
    "\n",
    "This formulation allows the network to combine past memory and new input in a gated, element-wise multiplicative way, enhancing expressivity over standard additive updates.\n",
    "\n",
    "Your Tasks:\n",
    "- Implement the new RNN cell in a `new_rnn_layer` class.\n",
    "- Choose training hyper-parameters (learning rate, batch size, hidden size, etc).\n",
    "- Your trained model must achieve a perplexity below 850 (i.e. exp(cross-entropy)) on the test set.  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baf3b848-d5c7-4bc1-ba06-c541c20be805",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1743869562272,
     "user": {
      "displayName": "Zehong Yan",
      "userId": "08549061813386001319"
     },
     "user_tz": -480
    },
    "id": "baf3b848-d5c7-4bc1-ba06-c541c20be805",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "fe04ed82-4c8d-4b81-e7ad-04b8d51c7cd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 25-04-14--18-56-16\n"
     ]
    }
   ],
   "source": [
    "# You must run this cell, but you don't need to expand it if you'd prefer not to see the details.\n",
    "\n",
    "%reset -f\n",
    "\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "def normalize_gradient(net):\n",
    "    grad_norm_sq=0\n",
    "    for p in net.parameters():\n",
    "        grad_norm_sq += p.grad.data.norm()**2\n",
    "    grad_norm=math.sqrt(grad_norm_sq)\n",
    "    if grad_norm<1e-4:\n",
    "        net.zero_grad()\n",
    "        print('grad norm close to zero')\n",
    "    else:\n",
    "        for p in net.parameters():\n",
    "             p.grad.data.div_(grad_norm)\n",
    "    return grad_norm\n",
    "\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('There are {} ({:.2f} million) parameters in this neural network'.format(\n",
    "        nb_param, nb_param/1e6))\n",
    "\n",
    "def sentence2vector(sentence):\n",
    "    words = sentence.split()\n",
    "    x = torch.LongTensor(len(words),1)\n",
    "    for idx, word in enumerate(words):\n",
    "         if word not in word2idx:\n",
    "            print('You entered a word which is not in the vocabulary.')\n",
    "            print('Make sure that you do not have any capital letters')\n",
    "         else:\n",
    "            x[idx,0]=word2idx[word]\n",
    "    return x\n",
    "\n",
    "def show_next_word(scores):\n",
    "    num_word_display=30\n",
    "    prob=F.softmax(scores,dim=2)\n",
    "    p=prob[-1].squeeze()\n",
    "    p,word_idx = torch.topk(p,num_word_display)\n",
    "    for i,idx in enumerate(word_idx):\n",
    "        percentage= p[i].item()*100\n",
    "        word=  idx2word[idx.item()]\n",
    "        print(  \"{:.1f}%\\t\".format(percentage),  word )\n",
    "\n",
    "def eval_on_test_set(test_data):\n",
    "    running_loss=0\n",
    "    num_batches=0\n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "    for count in range(0, test_data.shape[0]-1-seq_length, seq_length):\n",
    "        minibatch_data =  test_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n",
    "        scores, h  = net( minibatch_data, h )\n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length )\n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)\n",
    "        loss = criterion(  scores ,  minibatch_label )\n",
    "        h=h.detach()\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    total_loss = running_loss/num_batches\n",
    "    print('test: exp(loss) = ', math.exp(total_loss)  )\n",
    "    return math.exp(total_loss)\n",
    "\n",
    "# Load the PTB data\n",
    "path_data = 'datasets/' \n",
    "word2idx  =  torch.load(os.path.join(path_data, 'word2idx.pt'))\n",
    "idx2word  =  torch.load(os.path.join(path_data, 'idx2word.pt'))\n",
    "mini_train_size = 1000\n",
    "mini_test_size = 250\n",
    "train_data = torch.load(os.path.join(path_data, 'train_data.pt'))[:mini_train_size]\n",
    "test_data = torch.load(os.path.join(path_data, 'test_data.pt'))[:mini_test_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "--ttoY6KuskC",
   "metadata": {
    "id": "--ttoY6KuskC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch= 0 \t time= 2.931614875793457 \t lr= 1 \t exp(loss)= 2977.211918494641\n",
      "test: exp(loss) =  1579.1335663025004\n",
      "\n",
      "epoch= 1 \t time= 5.836550712585449 \t lr= 1 \t exp(loss)= 1840.5022768268361\n",
      "test: exp(loss) =  1015.1690695001333\n",
      "\n",
      "epoch= 2 \t time= 8.820572853088379 \t lr= 1 \t exp(loss)= 687.3804331232899\n",
      "test: exp(loss) =  1016.2288757809229\n",
      "\n",
      "epoch= 3 \t time= 12.08574891090393 \t lr= 1 \t exp(loss)= 461.0606639428799\n",
      "test: exp(loss) =  749.4604984346275\n",
      "Well Done!\n"
     ]
    }
   ],
   "source": [
    "class new_rnn_layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.R = nn.Linear( hidden_size , hidden_size )\n",
    "        self.V = nn.Linear( hidden_size , hidden_size )\n",
    "    def forward(self, g_seq , h_init ):\n",
    "        ##########################\n",
    "        # YOUR CODE STARTS HERE\n",
    "        seq_len, batch_size, hidden_size = g_seq.shape\n",
    "\n",
    "        # Precompute V * g_t for all t (can be done in parallel)\n",
    "        V_g = self.V(g_seq)  # shape: (seq_len, batch_size, hidden_size)\n",
    "        h_t = h_init.squeeze(0)  # shape: (batch_size, hidden_size)\n",
    "        h_seq = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            g_transformed = V_g[t]                 # V g_t\n",
    "            h_transformed = self.R(h_t)            # R h_{t-1}\n",
    "            h_t = torch.relu(h_transformed * g_transformed)  # apply recurrence with element wise\n",
    "            h_seq.append(h_t.unsqueeze(0))         # add time dimension back\n",
    "\n",
    "        h_seq = torch.cat(h_seq, dim=0)  # (seq_len, batch_size, hidden_size)\n",
    "        h_final = h_t.unsqueeze(0)       # (1, batch_size, hidden_size)\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "        ##########################\n",
    "        return h_seq , h_final\n",
    "\n",
    "class vanilla_rnn(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.layer2 = new_rnn_layer(hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, vocab_size)\n",
    "    def forward(self, word_seq, h_init ):\n",
    "        g_seq               =   self.layer1( word_seq )\n",
    "        h_seq , h_final     =   self.layer2( g_seq , h_init )\n",
    "        score_seq           =   self.layer3( h_seq )\n",
    "        return score_seq,  h_final\n",
    "        \n",
    "\n",
    "# Hyper-parameters\n",
    "bs = 20\n",
    "vocab_size = 10000\n",
    "seq_length = 25\n",
    "##########################\n",
    "# YOUR CODE STARTS HERE\n",
    "hidden_size = 300\n",
    "num_epochs = 10\n",
    "my_lr = 1\n",
    "# YOUR CODE ENDS HERE\n",
    "##########################\n",
    "# Initialize the net\n",
    "net = vanilla_rnn(vocab_size, hidden_size)\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD( net.parameters() , lr=my_lr )\n",
    "# Train the RNN Model\n",
    "start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss=0\n",
    "    num_batches=0\n",
    "    h = torch.zeros(1, bs, hidden_size)\n",
    "    for count in range(0, train_data.shape[0]-1-seq_length, seq_length):\n",
    "        optimizer.zero_grad()\n",
    "        minibatch_data =  train_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = train_data[ count+1 : count+seq_length+1 ]\n",
    "        h=h.detach()\n",
    "        h=h.requires_grad_()\n",
    "        scores, h  = net( minibatch_data, h )\n",
    "        scores          =            scores.view(  bs*seq_length , vocab_size)\n",
    "        minibatch_label =   minibatch_label.view(  bs*seq_length )\n",
    "        loss = criterion(  scores ,  minibatch_label )\n",
    "        loss.backward()\n",
    "        normalize_gradient(net)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    total_loss = running_loss/num_batches\n",
    "    if total_loss>10.0: total_loss = 10.0\n",
    "    elapsed = time.time()-start\n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
    "    test_loss = eval_on_test_set(test_data)\n",
    "    # Check if the test loss is small enough\n",
    "    if test_loss < 850:\n",
    "        print(\"Well Done!\")\n",
    "        break\n",
    "\n",
    "if test_loss > 850:\n",
    "    print(\"Try again.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5598e625-5c9b-4584-8031-b10b65b04259",
   "metadata": {
    "id": "5598e625-5c9b-4584-8031-b10b65b04259"
   },
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gGP5XryF8Qq3",
   "metadata": {
    "id": "gGP5XryF8Qq3"
   },
   "source": [
    "## Exercise 2 : Add Coupled External Modulation to LSTM\n",
    "\n",
    "In this exercise, you will enhance the standard LSTM layer by incorporating an **external modulation mechanism**. \n",
    "\n",
    "This upgraded architecture introduces an additional control signal that dynamically influences the hidden state through a learned gating mechanism.\n",
    "\n",
    "Precisely, this extended LSTM includes the usual components of an LSTM cell - input, forget, and output gates - but introduces two key changes:\n",
    "1. Coupled Input and Forget Gates: The input and forget gates are tied such that:\n",
    "   $$ \\eta_t = 1-\\theta_t $$\n",
    "2. External Modulation with Control Input: A new sequence $u_t$ (same length as the input sequence $g_t$) is used to modulate the final hidden state using a **learnable modulation gate**.\n",
    "\n",
    "The full equations of the new modulated LSTM are:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\tilde{h}_t = \\tanh ( Rh_{t-1} + b_R + V g_t + b_V) \\\\\n",
    "&c_t = \\theta_t \\odot c_{t-1} + \\eta_t  \\odot \\tilde{h}_t  \\hspace{2cm} \\textrm{ long-term memory state }\\\\\n",
    "&\\bar{h}_t = \\psi_t \\odot \\tanh (c_t)  \\quad\\quad\\quad  \\\\\n",
    "&h_t = \\gamma_t \\odot \\bar{h}_t + (1-\\gamma_t) \\odot u_t \\hspace{1cm} \\textrm{ (new) short-term memory state }  \\\\\\\\\n",
    "&\\textrm{with the gates are defined as}\\\\\\\\\n",
    "&\\quad \\theta_t = \\textrm{sigmoid}(Ah_{t-1} + b_A + Bg_t + b_B) \\quad \\textrm{ (forget gate) }\\\\\n",
    "&\\quad \\eta_t = 1-\\theta_t \\hspace{5.5cm} \\textrm{ (input gate) }\\\\\n",
    "&\\quad \\psi_t = \\textrm{sigmoid}(Eh_{t-1} + b_E + Fg_t + b_F) \\quad \\textrm{ (output gate) }\\\\\n",
    "&\\quad \\gamma_t = \\textrm{sigmoid}(Qh_{t-1} + b_Q + Su_t + b_S) \\quad \\textrm{ (modulation gate) }\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "where:\n",
    "- $g_t$ is the word input after embedding at time step $t$,\n",
    "- $u_t$ is the external modulation signal at time step $t$,\n",
    "- $A, B, E, F, Q, R, S, V$ are learnable weight matrices,\n",
    "- $b_A, b_B, b_E, b_F, b_Q, b_R, b_S, b_V$ are learnable biases,\n",
    "- $\\odot$ denotes the element-wise matrix multiplication.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee5d8c7b-4011-4554-9420-bd64abe67b34",
   "metadata": {
    "id": "ee5d8c7b-4011-4554-9420-bd64abe67b34",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 25-04-14--19-37-45\n"
     ]
    }
   ],
   "source": [
    "# You must run this cell, but you don't need to expand it if you'd prefer not to see the details.\n",
    "\n",
    "%reset -f\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "input_dict = torch.load('datasets/lstm_input.pt')\n",
    "w_t = input_dict[\"word_seq\"]\n",
    "u_t = input_dict[\"control_seq\"]\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.manual_seed(seed)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "gt_output = torch.load('datasets/lstm_output.pt')\n",
    "score_seq_gt = gt_output[\"score_seq\"]\n",
    "h_final_gt = gt_output[\"h_final\"]\n",
    "c_final_gt = gt_output[\"c_final\"]\n",
    "\n",
    "vocab_size = 100; hidden_size = 64\n",
    "\n",
    "h_init = torch.zeros(1, 4, hidden_size)\n",
    "c_init = torch.zeros(1, 4, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b3b13615-f5c0-4111-a31d-0c9dcbefd5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 4, 100])\n",
      "torch.Size([1, 1, 4, 64])\n",
      "torch.Size([1, 4, 64])\n",
      "score_seq incorrect -- Try again.\n",
      "h_final incorrect -- Try again.\n",
      "c_final incorrect -- Try again.\n"
     ]
    }
   ],
   "source": [
    "class LSTM_modulated(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.R = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.A = nn.Linear(hidden_size, hidden_size)\n",
    "        self.B = nn.Linear(hidden_size, hidden_size)\n",
    "        self.E = nn.Linear(hidden_size, hidden_size)\n",
    "        self.F = nn.Linear(hidden_size, hidden_size)\n",
    "        # For modulation mechanism\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.S = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, g_seq, u_seq, hc_init):\n",
    "        h_init, c_init = hc_init\n",
    "        h_t_pre = h_init\n",
    "        c_t_pre = c_init\n",
    "        ##########################\n",
    "        # YOUR CODE STARTS HERE\n",
    "        # h_t = h_t_pre.squeeze(0)\n",
    "        # c_t = c_t_pre.squeeze(0)\n",
    "\n",
    "        h_t = h_t_pre#.squeeze(0)\n",
    "        c_t = c_t_pre#.squeeze(0)\n",
    "\n",
    "        V_g = self.V(g_seq)\n",
    "        B_g = self.B(g_seq)\n",
    "        F_g = self.F(g_seq)\n",
    "        S_u = self.S(u_seq)\n",
    "        # YOUR CODE ENDS HERE\n",
    "        ##########################\n",
    "        h_seq = []\n",
    "        num_t = g_seq.size(0)\n",
    "        for t in range(num_t):\n",
    "            ##########################\n",
    "            # YOUR CODE STARTS HERE\n",
    "            A_h = self.A(h_t)\n",
    "            E_h = self.E(h_t)\n",
    "            R_h = self.R(h_t)\n",
    "            Q_h = self.Q(h_t)\n",
    "\n",
    "            # Gates\n",
    "            theta_t = torch.sigmoid(A_h + B_g[t])   # forget gate\n",
    "            # print(f'theta shape: {theta_t.shape}')\n",
    "            eta_t   = 1 - theta_t   # input gate    # input gate\n",
    "            # eta_t   = torch.ones(theta_t.shape[0], theta_t.shape[1], theta_t.shape[2]) - theta_t   # input gate    # input gate\n",
    "            # print(eta_t)\n",
    "            psi_t   = torch.sigmoid(E_h + F_g[t])   # output gate\n",
    "            # print(psi_t.shape)\n",
    "            phi_t = torch.sigmoid(Q_h + S_u[t]) #modulation gate\n",
    "\n",
    "            #candidates\n",
    "            h_tilde = torch.tanh(R_h + V_g[t])\n",
    "            c_t = (theta_t * c_t) + (eta_t * h_tilde) #long term memory state\n",
    "\n",
    "            h_bar = psi_t * torch.tanh(c_t)\n",
    "\n",
    "            # Hidden update\n",
    "            # h_t = (phi_t * h_bar) + ((torch.ones(phi_t.shape[0], phi_t.shape[1], phi_t.shape[2])-phi_t)*h_bar)\n",
    "            h_t = (phi_t * h_bar) + ((1-phi_t)*h_bar)\n",
    "\n",
    "            # YOUR CODE ENDS HERE\n",
    "            ##########################\n",
    "            h_seq.append(h_t.unsqueeze(0))\n",
    "            h_t_pre = h_t#.unsqueeze(0)\n",
    "            c_t_pre = c_t\n",
    "        h_seq = torch.cat(h_seq, dim=0)#.unsqueeze(0)\n",
    "        h_final = h_seq[-1, :, :].unsqueeze(0)\n",
    "        c_final = c_t#.unsqueeze(1)\n",
    "        return h_seq, (h_final, c_final)\n",
    "\n",
    "class three_layer_recurrent_net_modulated(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(vocab_size, hidden_size)\n",
    "        self.layer1_alt = nn.Linear(vocab_size, hidden_size)\n",
    "        self.layer2 = LSTM_modulated(hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, word_seq, control_seq, h_init, c_init):\n",
    "        g_seq = self.layer1(word_seq)          # main input\n",
    "        u_seq = self.layer1_alt(control_seq)   # control signal\n",
    "        h_seq, (h_final, c_final) = self.layer2(g_seq, u_seq, (h_init, c_init))\n",
    "        score_seq = self.layer3(h_seq)\n",
    "        return score_seq, h_final, c_final\n",
    "\n",
    "\n",
    "model = three_layer_recurrent_net_modulated(vocab_size, hidden_size)\n",
    "model.apply(init_weights)\n",
    "\n",
    "score_seq, h_final, c_final = model(w_t, u_t, h_init, c_init)\n",
    "print(score_seq.shape)\n",
    "print(h_final.shape)\n",
    "print(c_final.shape)\n",
    "\n",
    "if torch.allclose(score_seq_gt, score_seq, atol=1e-3):\n",
    "    print(\"score_seq correct -- Well Done!\")\n",
    "else:\n",
    "    print(\"score_seq incorrect -- Try again.\") \n",
    "if torch.allclose(h_final_gt, h_final, atol=1e-3):\n",
    "    print(\"h_final correct -- Well Done!\")\n",
    "else:\n",
    "    print(\"h_final incorrect -- Try again.\")\n",
    "if torch.allclose(c_final_gt, c_final, atol=1e-3):\n",
    "    print(\"c_final correct -- Well Done!\")\n",
    "else:\n",
    "    print(\"c_final incorrect -- Try again.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9140dece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 4, 100])\n"
     ]
    }
   ],
   "source": [
    "print(score_seq_gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84ff719c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "print(h_final_gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9429f59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "print(c_final_gt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ad8f3e-124b-4bd2-8719-f6a7646116c6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43a94a7c-cd8b-4a62-ab5e-cdbc435c7b26",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25cc632-e196-4b81-bf05-613012d2cc01",
   "metadata": {},
   "source": [
    "## Exercise 3 : Implement Gated Attention\n",
    "\n",
    "In this exercise, you will replace the standard softmax-based attention with a gated attention mechanism, where attention weights are computed using a sigmoid activation instead of Softmax.\n",
    "\n",
    "As a reminder, the standard attention mechanism with masking is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textrm{Standard-HA}(Q,K,V) &=  \\textrm{Softmax}\\Big( \\frac{Q K^T}{\\sqrt{d_\\textrm{head}}} \\odot \\textrm{Mask}  \\Big) V, \\\\\n",
    "&\\textrm{with } Q, K, V\\in \\mathbb{R}^{L\\times d_\\textrm{head}}, \\textrm{Mask}\\in\\mathbb{R}^{L\\times L}, \\\\\n",
    "& \\textrm{and }\\textrm{Mask}_{ij}= \n",
    "\\left\\{\n",
    "\\begin{array}{lll}\n",
    "1 & \\textrm{ if attention between $i$ and $j$ }\\\\\n",
    "- \\infty & \\textrm{ if no attention }\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Where:\n",
    "- L is the sequence length,\n",
    "- $d_\\textrm{head}$ is the embedding dimension,\n",
    "- and the Mask restricts attention.\n",
    "\n",
    "In the new design of gated attention, attention weights are computed using a sigmoid function (sigmoid provides smoother attention scores), and are explicitly normalized using a normalization constant Z. The gated attention is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textrm{Gated-HA}(Q,K,V) &=  \\frac{1}{Z} \\odot \\textrm{Sigmoid}\\Big( \\frac{Q K^T}{\\sqrt{d_\\textrm{head}}} \\odot \\textrm{Mask}  \\Big) V, \\\\\n",
    "&\\textrm{The terms $Q, K, V \\textrm{and Mask}$ are as defined above.} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\textrm{Z ensures the attention weights sum to 1 row-wise (like Softmax). Thus, matrix Z is defined as:}\\\\ \n",
    "& Z_{i,j} = \\sum_{j'=1}^L A_{i,j'} + \\varepsilon, \\ \\forall i,j, \\textrm{ with } \\varepsilon=0.001 \\textrm{ and } A=\\textrm{Sigmoid}\\Big( \\frac{Q K^T}{\\sqrt{d_\\textrm{head}}} \\odot \\textrm{Mask}  \\Big)\\in\\mathbb{R}^{L\\times L} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Your Tasks:\n",
    "- Implement the new gated attention mechanism in the `Gated_AttentionHead` class.\n",
    "- Choose training hyper-parameters (hidden size, num_heads, num_blocks).\n",
    "- Your trained model must achieve a perplexity below 950 (i.e. exp(cross-entropy)) on the test set.\n",
    "  \n",
    "\n",
    "**Hints**:\n",
    "- Apply the sigmoid function to the masked attention scores: $Q K^T / \\sqrt{d_\\textrm{head}} \\odot \\textrm{Mask} $.\n",
    "- Compute the row-wise sum for normalization. You may use `torch.sum(input, dim)`. Example:\n",
    "```\n",
    "input_tensor = torch.tensor([[1, 2, 3],\n",
    "                             [4, 5, 6]])\n",
    "print('Tensor size:', input_tensor.size()) # Output: torch.Size([2, 3])\n",
    "# Sum along dimension 1 (across columns), keeping the dimension 1:\n",
    "sum_keepdim = torch.sum(input_tensor, dim=1, keepdim=True)\n",
    "print('Tensor size:', sum_keepdim.size()) # Output: torch.Size([2, 1])\n",
    "print(\"Row-wise sum with keepdim=True:\\n\", sum_keepdim) # Output: tensor([[ 6],[15]])\n",
    "```\n",
    "- Broadcast the division of the row-wise sum to each row of the attention matrix (so that each row is properly normalized to 1). Example:\n",
    "```\n",
    "attention_scores = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                                 [4.0, 5.0, 6.0]])\n",
    "# Step 1: Row-wise sum (sum over dim=1)\n",
    "row_sums = torch.sum(attention_scores, dim=1, keepdim=True)\n",
    "# Step 2: Broadcast division for normalization\n",
    "normalized_attention = attention_scores / row_sums\n",
    "```\n",
    "- Add a small $\\varepsilon=0.001$ to avoid division by zero.\n",
    "- As usual, multiply the normalized attention matrix by matrix V.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "48e22746-a26e-4b32-9d07-3af13b0cb377",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 25-04-14--20-03-19\n"
     ]
    }
   ],
   "source": [
    "# You must run this cell, but you don't need to expand it if you'd prefer not to see the details.\n",
    "\n",
    "%reset -f\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "mini_train_size = 1000\n",
    "mini_test_size = 250\n",
    "train_data = torch.load(os.path.join('datasets/train_data.pt'))[:mini_train_size]\n",
    "test_data = torch.load(os.path.join('datasets/test_data.pt'))[:mini_test_size]\n",
    "\n",
    "bs = 20\n",
    "vocab_size = 10000\n",
    "\n",
    "def generate_positional_encoding(seq_length, dim):\n",
    "    assert dim == 2* (dim//2) # check if dim is divisible by 2\n",
    "    pe = torch.zeros(seq_length, dim)\n",
    "    position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / dim))\n",
    "    pe[:,0::2] = torch.sin(position * div_term)\n",
    "    pe[:,1::2] = torch.cos(position * div_term)\n",
    "    return pe  \n",
    "\n",
    "def eval_on_test_set(test_data):\n",
    "    net.eval()\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "    for count in range(0, test_data.shape[0]-1-seq_length, seq_length):       \n",
    "        minibatch_data =  test_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = test_data[ count+1 : count+seq_length+1 ]\n",
    "        pos = generate_positional_encoding(seq_length, hidden_size)\n",
    "        scores = net( minibatch_data, pos )\n",
    "        minibatch_label = minibatch_label.view(  bs*seq_length ) \n",
    "        scores = scores.view(  bs*seq_length , vocab_size)\n",
    "        loss = criterion(scores, minibatch_label) \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1        \n",
    "    total_loss = running_loss/num_batches\n",
    "    print('test: exp(loss) = ', math.exp(total_loss)  )\n",
    "    return math.exp(total_loss)\n",
    "\n",
    "\n",
    "class MultipleAttentionHead(nn.Module):\n",
    "    def __init__(self, d, num_heads):\n",
    "        super().__init__()\n",
    "        d_head = d // num_heads # dim_head = d // num_heads, usually dimension per head is 64\n",
    "        assert d == d_head * num_heads # check divisibility\n",
    "        self.MHA = nn.ModuleList([ Gated_AttentionHead(d, d_head) for _ in range(num_heads) ])\n",
    "        self.WO = nn.Linear(d, d) # combination layer\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self, H): # size(H)=[batch_size, seq_length, d]\n",
    "        batch_size = H.size(0); seq_length = H.size(1)\n",
    "        H_heads = []\n",
    "        for HA_layer in self.MHA:\n",
    "            H_heads.append(HA_layer(H)) # size=[batch_size, seq_length, d_head]\n",
    "        H_heads = torch.cat(H_heads, dim=2) # size=[batch_size, seq_length, d]            \n",
    "        H_heads = self.dropout(H_heads) # dropout attention activations\n",
    "        H = self.WO(H_heads) # size=[batch_size, seq_length, d]\n",
    "        return H\n",
    "        \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d, num_heads):\n",
    "        super().__init__()\n",
    "        self.LN_MHA = nn.LayerNorm(d)\n",
    "        self.LN_MLP = nn.LayerNorm(d)\n",
    "        self.MHA = MultipleAttentionHead(d, num_heads)\n",
    "        self.MLP = nn.Sequential(nn.Linear(d,4*d), nn.ReLU(), nn.Dropout(0.5), nn.Linear(4*d,d))        \n",
    "    def forward(self, H): # size=[batch_size, seq_length, d]\n",
    "        # Multiple Attention Heads w/ layer normalization (LN), residual connection (RC)\n",
    "        H = H + self.MHA(self.LN_MHA(H)) # size=[batch_size, seq_length, d]\n",
    "        # MLP w/ layer normalization (LN), residual connection (RC)\n",
    "        H = H + self.MLP(self.LN_MLP(H)) # size=[batch_size, seq_length, d]\n",
    "        return H # size=[batch_size, seq_length, d]\n",
    "        \n",
    "class Transformer_decoder(nn.Module):\n",
    "    def __init__(self, d, num_heads, num_blocks, seq_length):\n",
    "        super().__init__()\n",
    "        self.TR_Blocks = nn.ModuleList([ TransformerBlock(d, num_heads) for _ in range(num_blocks) ]) \n",
    "    def forward(self, batch_seq, pos_enc):\n",
    "        H = batch_seq.transpose(1,0) # size=[batch_size, seq_length, d]\n",
    "        batch_size = H.size(0); batch_len = H.size(1)\n",
    "        # Add positional encoding  \n",
    "        pos_enc = pos_enc.unsqueeze(dim=0) # size=[1,          seq_length, d]\n",
    "        H = H + pos_enc                    # size=[batch_size, seq_length, d]\n",
    "        # Apply transformer blocks \n",
    "        for TR_Block in self.TR_Blocks:\n",
    "            H = TR_Block(H)\n",
    "        # Output\n",
    "        H = H.permute(1,0,2)  # size=[batch_length, batch_size, d]\n",
    "        return H # return prediction scores for next token\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, d, num_heads, num_blocks, seq_length):\n",
    "        super(ANN, self).__init__()\n",
    "        self.decoder = Transformer_decoder(d, num_heads, num_blocks, seq_length)\n",
    "    def forward(self, g_seq , pos ):\n",
    "        h_dec_seq = self.decoder( g_seq , pos )\n",
    "        return h_dec_seq \n",
    "    \n",
    "class attention_net(nn.Module):\n",
    "    def __init__(self, d, num_heads, num_blocks, seq_length):\n",
    "        super(attention_net, self).__init__()  \n",
    "        self.layer1 = nn.Embedding( vocab_size  , hidden_size  )\n",
    "        self.layer2 = ANN(d, num_heads, num_blocks, seq_length)\n",
    "        self.layer3 = nn.Linear(    hidden_size , vocab_size   )\n",
    "    def forward(self, word_seq, pos ):\n",
    "        g_seq     =   self.layer1( word_seq ) # size=(seq_length, bs, hidden_dim) \n",
    "        h_seq     =   self.layer2( g_seq , pos ) # size=(seq_length, bs, hidden_dim) \n",
    "        score_seq =   self.layer3( h_seq ) # size=(seq_length, bs, vocab_size)\n",
    "        return score_seq \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "38fa2b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([[ 6.],\n",
      "        [15.]])\n",
      "tensor([[0.1667, 0.3333, 0.5000],\n",
      "        [0.2667, 0.3333, 0.4000]])\n"
     ]
    }
   ],
   "source": [
    "attention_scores = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                                 [4.0, 5.0, 6.0]])\n",
    "# Step 1: Row-wise sum (sum over dim=1)\n",
    "print(attention_scores)\n",
    "row_sums = torch.sum(attention_scores, dim=1, keepdim=True)\n",
    "print(row_sums)\n",
    "# Step 2: Broadcast division for normalization\n",
    "normalized_attention = attention_scores / row_sums\n",
    "print(normalized_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9d1002b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum() received an invalid combination of arguments - got (Tensor, keep_dim=bool, dim=int), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: keep_dim, dim\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print(x.view(x.shape(0)))\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(row)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print(row.shape)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# for i in range(row.shape[0]):\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     x[]\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: sum() received an invalid combination of arguments - got (Tensor, keep_dim=bool, dim=int), but expected one of:\n * (Tensor input, *, torch.dtype dtype)\n      didn't match because some of the keywords were incorrect: keep_dim, dim\n * (Tensor input, tuple of ints dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n * (Tensor input, tuple of names dim, bool keepdim, *, torch.dtype dtype, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [[1,2,3], \n",
    "     [1,2,3],\n",
    "     [1,2,3]], \n",
    "\n",
    "    [[1,2,3], \n",
    "     [1,2,3],\n",
    "     [1,2,3]],\n",
    "\n",
    "    ])\n",
    "print(x.shape)\n",
    "# print(x.view(x.shape(0)))\n",
    "\n",
    "row = torch.sum(x, dim = 1, keep_dim = True)\n",
    "print(row)\n",
    "# print(row.shape)\n",
    "\n",
    "# for i in range(row.shape[0]):\n",
    "#     x[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fd402ab4-7ba8-429a-99f6-f539393f86c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m minibatch_label \u001b[38;5;241m=\u001b[39m train_data[ count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m : count\u001b[38;5;241m+\u001b[39mseq_length\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m ]    \n\u001b[1;32m     59\u001b[0m pos \u001b[38;5;241m=\u001b[39m generate_positional_encoding(seq_length, hidden_size) \u001b[38;5;66;03m# size=(seq_length, hidden_dim)    \u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mminibatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# size=(seq_length, bs, vocab_size)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mview(  bs\u001b[38;5;241m*\u001b[39mseq_length , vocab_size) \u001b[38;5;66;03m# size=(seq_length/2.bs, vocab_size)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m minibatch_label \u001b[38;5;241m=\u001b[39m minibatch_label\u001b[38;5;241m.\u001b[39mview(  bs\u001b[38;5;241m*\u001b[39mseq_length ) \u001b[38;5;66;03m# size=(seq_length/2.bs, vocab_size)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearn_course/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearn_course/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[102], line 116\u001b[0m, in \u001b[0;36mattention_net.forward\u001b[0;34m(self, word_seq, pos)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, word_seq, pos ):\n\u001b[1;32m    115\u001b[0m     g_seq     \u001b[38;5;241m=\u001b[39m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1( word_seq ) \u001b[38;5;66;03m# size=(seq_length, bs, hidden_dim) \u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     h_seq     \u001b[38;5;241m=\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_seq\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# size=(seq_length, bs, hidden_dim) \u001b[39;00m\n\u001b[1;32m    117\u001b[0m     score_seq \u001b[38;5;241m=\u001b[39m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3( h_seq ) \u001b[38;5;66;03m# size=(seq_length, bs, vocab_size)\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m score_seq\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearn_course/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearn_course/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[102], line 105\u001b[0m, in \u001b[0;36mANN.forward\u001b[0;34m(self, g_seq, pos)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, g_seq , pos ):\n\u001b[0;32m--> 105\u001b[0m     h_dec_seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_seq\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h_dec_seq\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearn_course/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearn_course/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[102], line 95\u001b[0m, in \u001b[0;36mTransformer_decoder.forward\u001b[0;34m(self, batch_seq, pos_enc)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Apply transformer blocks \u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m TR_Block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTR_Blocks:\n\u001b[0;32m---> 95\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[43mTR_Block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Output\u001b[39;00m\n\u001b[1;32m     97\u001b[0m H \u001b[38;5;241m=\u001b[39m H\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# size=[batch_length, batch_size, d]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearn_course/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearn_course/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[102], line 78\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, H)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, H): \u001b[38;5;66;03m# size=[batch_size, seq_length, d]\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# Multiple Attention Heads w/ layer normalization (LN), residual connection (RC)\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     H \u001b[38;5;241m=\u001b[39m H \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMHA\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLN_MHA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# size=[batch_size, seq_length, d]\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# MLP w/ layer normalization (LN), residual connection (RC)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     H \u001b[38;5;241m=\u001b[39m H \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMLP(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLN_MLP(H)) \u001b[38;5;66;03m# size=[batch_size, seq_length, d]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearn_course/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearn_course/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[102], line 63\u001b[0m, in \u001b[0;36mMultipleAttentionHead.forward\u001b[0;34m(self, H)\u001b[0m\n\u001b[1;32m     61\u001b[0m H_heads \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m HA_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMHA:\n\u001b[0;32m---> 63\u001b[0m     H_heads\u001b[38;5;241m.\u001b[39mappend(\u001b[43mHA_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# size=[batch_size, seq_length, d_head]\u001b[39;00m\n\u001b[1;32m     64\u001b[0m H_heads \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(H_heads, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# size=[batch_size, seq_length, d]            \u001b[39;00m\n\u001b[1;32m     65\u001b[0m H_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(H_heads) \u001b[38;5;66;03m# dropout attention activations\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearn_course/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/deeplearn_course/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[130], line 26\u001b[0m, in \u001b[0;36mGated_AttentionHead.forward\u001b[0;34m(self, H)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(attention_score.shape)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m row_sums \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 26\u001b[0m normalized_attention \u001b[38;5;241m=\u001b[39m \u001b[43mattention_scores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_sums\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m H_HA \u001b[38;5;241m=\u001b[39m normalized_attention \u001b[38;5;241m@\u001b[39m V \u001b[38;5;66;03m# softmax( QK^T / sqrt(d) ) V, (B,L,L) @ (B,L,d) => (B,L,d), size=[batch_size, batch_length, d)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# YOUR CODE ENDS HERE\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m##########################\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "class Gated_AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self, d, d_head):\n",
    "        super().__init__()\n",
    "        self.LN_MHA = nn.LayerNorm(d_head)\n",
    "        self.LN_MLP = nn.LayerNorm(d_head)\n",
    "        self.query = nn.Linear(d, d_head, bias=False) # query embedding layer\n",
    "        self.key = nn.Linear(d, d_head, bias=False) # key embedding layer\n",
    "        self.value = nn.Linear(d, d_head) # value embedding layer\n",
    "        \n",
    "    def forward(self, H): # size(H)=[batch_size, seq_length, d]\n",
    "        batch_size = H.size(0); batch_len = H.size(1)\n",
    "        # Compute a single attention head H = Softmax( QK^T / d^0.5 ) V\n",
    "        Q = self.query(H) # size=[batch_size, batch_length, d]        \n",
    "        K = self.key(H) # size=[batch_size, batch_length, d]\n",
    "        V = self.value(H) # size=[batch_size, batch_length, d]\n",
    "        # Gated attention\n",
    "        ##########################\n",
    "        # YOUR CODE STARTS HERE\n",
    "        attention_score = Q @ K.transpose(2,1) * H.size(2)**-0.5 # QK^T/sqrt(d), (B,L,d) @ (B,d,L) => (B,L,L), size=[batch_size, batch_length, batch_length)\n",
    "        mask = torch.tril(torch.ones(batch_len,batch_len)).long() # mask to use previous tokens only : { token(<=t) }, size=[batch_len,batch_len]\n",
    "        attention_score = attention_score.masked_fill(mask==0, value=float('-inf')) # softmax(-inf)=0 prevents using next tokens for prediction, size=(batch_size, batch_len, batch_len)\n",
    "        attention_score = torch.sigmoid(attention_score) # sum weights = 1, size=[batch_size, batch_length, batch_len)\n",
    "        # print(attention_score.shape)\n",
    "        row_sums = torch.sum(attention_scores, dim=1, keepdim=False)\n",
    "        normalized_attention = attention_scores / (row_sums + 1e-6)\n",
    "        H_HA = normalized_attention @ V # softmax( QK^T / sqrt(d) ) V, (B,L,L) @ (B,L,d) => (B,L,d), size=[batch_size, batch_length, d)\n",
    "        # YOUR CODE ENDS HERE\n",
    "        ##########################\n",
    "        return H_HA # return prediction scores for next token\n",
    "\n",
    "\n",
    "# Instantiate network\n",
    "seq_length = 50\n",
    "##########################\n",
    "# YOUR CODE STARTS HERE\n",
    "hidden_size = 150\n",
    "num_heads = 10\n",
    "num_blocks = 3\n",
    "# YOUR CODE ENDS HERE\n",
    "##########################\n",
    "net = attention_net(hidden_size, num_heads, num_blocks, seq_length)\n",
    "pos = generate_positional_encoding(seq_length, hidden_size) # size=(seq_length, hidden_dim)\n",
    "\n",
    "# Optimization\n",
    "my_lr = 0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=my_lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train\n",
    "start = time.time()\n",
    "for epoch in range(10):\n",
    "    running_loss=0\n",
    "    num_batches=0    \n",
    "    for count in range(0, train_data.shape[0]-1-seq_length, seq_length):\n",
    "        optimizer.zero_grad()\n",
    "        minibatch_data = train_data[ count   : count+seq_length   ]\n",
    "        minibatch_label = train_data[ count+1 : count+seq_length+1 ]    \n",
    "        pos = generate_positional_encoding(seq_length, hidden_size) # size=(seq_length, hidden_dim)    \n",
    "        scores = net( minibatch_data, pos ) # size=(seq_length, bs, vocab_size)\n",
    "        scores = scores.view(  bs*seq_length , vocab_size) # size=(seq_length/2.bs, vocab_size)\n",
    "        minibatch_label = minibatch_label.view(  bs*seq_length ) # size=(seq_length/2.bs, vocab_size)\n",
    "        loss = criterion(scores, minibatch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    total_loss = running_loss/num_batches\n",
    "    elapsed = time.time()-start\n",
    "    \n",
    "    print('')\n",
    "    print('epoch=',epoch, '\\t time=', elapsed,'\\t lr=', my_lr, '\\t exp(loss)=',  math.exp(total_loss))\n",
    "    test_loss = eval_on_test_set(test_data) \n",
    "    \n",
    "    # Check if the test loss is small enough\n",
    "    if test_loss < 950:\n",
    "        print(\"Well Done!\")\n",
    "        break\n",
    "\n",
    "if test_loss > 950:\n",
    "    print(\"Try again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b249b0-9988-469a-bd03-ca606635387d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c05faeee-6028-4f2f-92ed-65278ad66372",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9865ced-8c30-46a9-9d4c-973e862b766f",
   "metadata": {
    "id": "c9865ced-8c30-46a9-9d4c-973e862b766f"
   },
   "source": [
    "## Exercise 4 : Implement Window Attention\n",
    "\n",
    "Window Attention is a variant of causal attention where each token attends only to a limited local window of preceding tokens, instead of all previous tokens in the sequence. This localized attention reduces computational overhead and focuses the model’s capacity on nearby context.\n",
    "\n",
    "In this exercise, you will implement window attention with a fixed window size of 3. This means that each token can only attend to itself and the two preceding tokens. See figure below: \n",
    "<center>\n",
    "<img src=\"pic/window_attention.png\" style=\"height:350px\"/>\n",
    "</center>\n",
    "\n",
    "As a reminder, we define the masked attention with the following formula:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\textrm{Mask-HA}(Q,K,V) &=  \\textrm{Softmax}\\Big( \\frac{Q K^T}{\\sqrt{d_\\textrm{head}}} \\odot \\textrm{Mask}  \\Big) V, \\\\\n",
    "&\\textrm{with } Q, K, V\\in\\mathbb{R}^{L\\times d_\\textrm{head}}, \\textrm{Mask}\\in\\mathbb{R}^{L\\times L}, \\\\\n",
    "& \\textrm{and }\\textrm{Mask}_{ij}= \n",
    "\\left\\{\n",
    "\\begin{array}{lll}\n",
    "1 & \\textrm{ if attention between $i$ and $j$ }\\\\\n",
    "- \\infty & \\textrm{ if no attention }\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Where:\n",
    "- L is the sequence length,\n",
    "- $d_\\textrm{head}$ is the embedding dimension,\n",
    "- and the Mask restricts attention.\n",
    "\n",
    "\n",
    "**Hints**:\n",
    "1. First, compute the standard attention score matrix: $Q K^T / \\sqrt{d_\\textrm{head}}$.\n",
    "2. Then apply the window mask to allow attention only within a window of size 3 of previous tokens.\n",
    "3. You may use `torch.tril()` to build the window mask. Examples:\n",
    "```\n",
    "a = torch.tensor([[ 1, 2, 3],\n",
    "                  [ 4, 5, 6],\n",
    "                  [ 7, 8, 9]])\n",
    "torch.tril(a, diagonal=0)\n",
    ">>> torch.tensor([[ 1, 0, 0],)\n",
    "                  [ 4, 5, 0],\n",
    "                  [ 7, 8, 9]])\n",
    "torch.tril(a, diagonal=1)\n",
    " >>> torch.tensor([[ 1, 2, 0],)\n",
    "                   [ 4, 5, 6],\n",
    "                   [ 7, 8, 9]])\n",
    "torch.tril(a, diagonal=-1)\n",
    ">>> torch.tensor([[ 0, 0, 0],)\n",
    "                  [ 4, 0, 0],\n",
    "                  [ 7, 8, 0]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "66e3a942-af2c-446f-a455-fbfdeaee3422",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "error",
     "timestamp": 1743841447782,
     "user": {
      "displayName": "Xiaokang Liu",
      "userId": "16731343253708449464"
     },
     "user_tz": -480
    },
    "id": "66e3a942-af2c-446f-a455-fbfdeaee3422",
    "outputId": "c5acbc0e-b4c9-4a56-e559-840b14176eca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 25-04-14--19-59-51\n",
      "Well Done!\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "class Window_Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d, d_head, window_size):\n",
    "        super().__init__()\n",
    "        self.LN_MHA = nn.LayerNorm(d_head)\n",
    "        self.LN_MLP = nn.LayerNorm(d_head)\n",
    "        self.query = nn.Linear(d, d_head, bias=False)\n",
    "        self.key = nn.Linear(d, d_head, bias=False)\n",
    "        self.value = nn.Linear(d, d_head)\n",
    "        self.window_size = window_size # new hyper-parameter\n",
    "    \n",
    "    def forward(self, H):\n",
    "        batch_size = H.size(0); batch_len = H.size(1)\n",
    "        ##########################\n",
    "        # YOUR CODE STARTS HERE\n",
    "        # Masked self-attention decoder\n",
    "        # Compute a single attention head H = Softmax( QK^T / d^0.5 ) V\n",
    "        Q = self.query(H) # size=[batch_size, batch_length, d] \n",
    "        K = self.key(H) # size=[batch_size, batch_length, d]\n",
    "        V = self.value(H) # size=[batch_size, batch_length, d]\n",
    "        attention_score = Q @ K.transpose(2,1) * H.size(2)**-0.5 # QK^T/sqrt(d), (B,L,d) @ (B,d,L) => (B,L,L), size=[batch_size, batch_length, batch_length)\n",
    "        mask = torch.tril(torch.ones(batch_len,batch_len)).long() # mask to use previous tokens only : { token(<=t) }, size=[batch_len,batch_len]\n",
    "        new_window_mask = torch.tril(torch.ones(batch_len,batch_len), diagonal = -1*window_size).long()\n",
    "        \n",
    "        attention_score = attention_score.masked_fill(mask==0, value=float('-inf')) # softmax(-inf)=0 prevents using next tokens for prediction, size=(batch_size, batch_len, batch_len)\n",
    "        attention_score = attention_score.masked_fill(new_window_mask==1, value=float('-inf')) # softmax(-inf)=0 prevents using next tokens for prediction, size=(batch_size, batch_len, batch_len)\n",
    "        # print(attention_score.shape)\n",
    "        # print(attention_score)\n",
    "        attention_score = torch.softmax(attention_score, dim=2) # sum weights = 1, size=[batch_size, batch_length, batch_len)\n",
    "        H_HA = attention_score @ V # softmax( QK^T / sqrt(d) ) V, (B,L,L) @ (B,L,d) => (B,L,d), size=[batch_size, batch_length, d)\n",
    "        return H_HA # return prediction scores for next token\n",
    "        # YOUR CODE ENDS HERE\n",
    "        ##########################\n",
    "        return H_HA\n",
    "\n",
    "\n",
    "# Instantiate the window attention layer\n",
    "d = 32\n",
    "d_head = 16\n",
    "window_size = 3\n",
    "WA = Window_Attention(d, d_head, window_size)\n",
    "\n",
    "# Evaluation of your window attention\n",
    "WA.load_state_dict(torch.load('datasets/wa_model_weights.pt')) # load pre-defined parameters of the window attention layer\n",
    "H_in = torch.load('datasets/wa_input.pt') # load pre-defined input tensor\n",
    "H_out = WA(H_in) # output of forward pass \n",
    "H_gt = torch.load('datasets/wa_gt.pt') # exact output solution\n",
    "if torch.sum(torch.abs(H_out - H_gt)) < 1e-4: # check output of forward pass w.r.t. exact solution\n",
    "    print(\"Well Done!\")\n",
    "else:\n",
    "    print(\"Try Again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf7c6f1-6a07-4212-8f60-0cb4877d2360",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97075410-88da-48bb-9bc8-43caf1b9b8cd",
   "metadata": {
    "id": "923e5296-4193-4095-b2ca-a29603a91803"
   },
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a695d9-1252-40d8-ac6a-398db753c330",
   "metadata": {
    "id": "20a695d9-1252-40d8-ac6a-398db753c330"
   },
   "source": [
    "## Exercise 5 : Vectorized Multi-Head Attention\n",
    "\n",
    "During a tutorial, you implemented Multi-Head Attention (MHA) by explicitly **looping** over each attention head. \n",
    "\n",
    "While this approach is functionally correct, it is computationally inefficient and not scalable for larger models.\n",
    "\n",
    "Your goal in this exercise is to re-implement Multi-Head Attention in a fully vectorized form — **without** using any for loops, i.e. all heads are processed in parallel.\n",
    "\n",
    "\n",
    "**Hints:**\n",
    "- For each of the query, key and value matrix transformations, you may use an implementation that only requires a single $d\\times d$ linear transformation, instead of $\\textrm{num\\_heads}$ linear transformations of size  $\\ d \\times (d/\\textrm{num\\_heads})$ . \n",
    "- Use a bias (`bias=True`) for the query, key and value linear transformations.\n",
    "- You may use `torch.reshape(input, shape)` that returns a tensor with the same data and same number of elements as `input`, but with the specified `shape`. Example:\n",
    "```\n",
    "x = torch.tensor([[1, 2, 3, 4, 5, 6],\n",
    "                  [7, 8, 9, 10, 11, 12]])\n",
    "print(\"Original shape:\", x.size())\n",
    "# Output: torch.Size([2, 6])\n",
    "y = torch.reshape(x, (3, 4))\n",
    "print(y)\n",
    "print(\"New shape:\", y.size())\n",
    "# New shape: torch.Size([3, 4])\n",
    "```\n",
    "- You may use `torch.permute(input, dims)` that returns the original tensor input but with its dimensions permuted. Example:\n",
    "```\n",
    "x = torch.randn(2, 3, 4)  # e.g., (batch, channels, features)\n",
    "print(\"Original shape:\", x.size())\n",
    "# Output: torch.Size([2, 3, 4])\n",
    "# Permute dimensions to shape (3, 2, 4)\n",
    "y = x.permute(1, 0, 2)\n",
    "print(\"Permuted shape:\", y.size())\n",
    "# Output: torch.Size([3, 2, 4])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c57b40a-d2ca-48ca-8d17-0e886a4e67f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 714,
     "status": "ok",
     "timestamp": 1743926564274,
     "user": {
      "displayName": "Chew Kin whye",
      "userId": "16899899049406240390"
     },
     "user_tz": -480
    },
    "id": "3c57b40a-d2ca-48ca-8d17-0e886a4e67f2",
    "outputId": "8de38fc0-ae13-4b3e-a2fd-960008bbba00"
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "class your_MultipleAttentionHead(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, d, num_heads):\n",
    "        \n",
    "        super().__init__()\n",
    "        ##########################\n",
    "        # YOUR CODE STARTS HERE\n",
    "        self.num_heads = num_heads\n",
    "        self.d_head = d // num_heads\n",
    "        assert d == self.d_head * self.num_heads # check divisibility\n",
    "        # Instead of have num_heads matrices of size (d, d_head), we simply use a single (d, d) matrix, and each head would take their respective dimensions\n",
    "        self.query =   # query embedding layer\n",
    "        self.key   =   # key embedding layer\n",
    "        self.value =   # value embedding layer\n",
    "        # YOUR CODE ENDS HERE\n",
    "        ##########################\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        ##########################\n",
    "        # YOUR CODE STARTS HERE\n",
    "        H = \n",
    "        # YOUR CODE ENDS HERE\n",
    "        ##########################\n",
    "        return H\n",
    "\n",
    "\n",
    "batch_size, seq_length, d, num_heads = 32, 100, 256, 8\n",
    "x = torch.randn(batch_size, seq_length, d)\n",
    "\n",
    "your_implementation = your_MultipleAttentionHead(d, num_heads)\n",
    "\n",
    "\n",
    "# Evaluate your implementation vs. PyTorch implementation\n",
    "correct_implementation = torch.nn.MultiheadAttention(embed_dim=d, num_heads=num_heads, bias=True, batch_first=True)\n",
    "with torch.no_grad():\n",
    "    # Copy-paste linear matrices from your implementation to the PyTorch implementation\n",
    "    in_proj_weight_custom = torch.cat([your_implementation.query.weight, your_implementation.key.weight, your_implementation.value.weight], dim=0)\n",
    "    correct_implementation.in_proj_weight.copy_(in_proj_weight_custom)\n",
    "    # Copy-paste linear biases from your implementation to PyTorch implementation\n",
    "    in_proj_bias_custom = torch.cat([your_implementation.query.bias, your_implementation.key.bias, your_implementation.value.bias], dim=0)\n",
    "    correct_implementation.in_proj_bias.copy_(in_proj_bias_custom)\n",
    "    # Make the output projection layer of the PyTorch implementation as Identity transformation\n",
    "    correct_implementation.out_proj.weight.copy_(torch.eye(d))\n",
    "    correct_implementation.out_proj.bias.zero_()\n",
    "if torch.allclose(your_implementation(x, x, x), correct_implementation(x, x, x)[0], atol=1e-3):\n",
    "    print(\"Well Done!\")\n",
    "else:\n",
    "    print(\"Try Again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb81c4d-cbd0-4408-b345-4d49083dc164",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3421c4b-4ad6-4dfa-a629-2baf5e76dd63",
   "metadata": {
    "id": "d3421c4b-4ad6-4dfa-a629-2baf5e76dd63"
   },
   "source": [
    "## End of coding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab4291-f8f9-4188-8bcc-1217c6b5baed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deeplearn_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
