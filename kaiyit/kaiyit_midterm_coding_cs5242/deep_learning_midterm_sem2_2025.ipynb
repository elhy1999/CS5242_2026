{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46bcc520-6328-4d85-acae-499775e62f2c",
   "metadata": {
    "id": "46bcc520-6328-4d85-acae-499775e62f2c"
   },
   "source": [
    "## CS5242 Neural Networks and Deep Learning\n",
    "## Sem 2 2024/25\n",
    "### Lecturer: Xavier Bresson\n",
    "### Teaching Assistants: Liu Nian, Liu Ziming, Liu Xiaokang, Yan Zehong, Chew KinWhye\n",
    "\n",
    "## Midterm exam, coding test\n",
    "Date: Mar 3 2025 <br>\n",
    "Time: 6:45pm-8:15pm (90min) <br>\n",
    "\n",
    "*Instructions* <br>\n",
    "Name: Please, add your name here : KOH KAI YIT<br>\n",
    "Answers: Please write your answers directly in this notebook by completing the code sections marked with  \n",
    "`# YOUR CODE STARTS HERE`  \n",
    "`# YOUR CODE` (it can span one or multiple lines)  \n",
    "`# YOUR CODE ENDS HERE`. <br>\n",
    "Remark: Ensure your code runs without errors. No points will be awarded for buggy or incomplete code.  \n",
    "Remark: If certain conditions of the questions (for eg. hyperparameter values) are not stated, you are free to choose anything you want.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LYZhVNCs2kZV",
   "metadata": {
    "id": "LYZhVNCs2kZV"
   },
   "source": [
    "## Exercise 1 : Implement Softmax function and Cross-entropy loss.\n",
    "\n",
    "The softmax function converts a vector of real numbers into a probability distribution vector, and the cross-entropy loss measures the difference between two probability distributions by quantifying how well the predicted probability distribution matches the true labels.\n",
    "\n",
    "Your task is to implement these two basic functions by yourself without using PyTorch, TensorFlow, or any built-in softmax or cross-entropy loss functions. You can use either loop or tensor computation to implement.\n",
    "\n",
    "Ensure that the final output is \"Well Done!\".\n",
    "\n",
    "Input:\n",
    "\n",
    "`x`: A 2D tensor of shape (batch_size, vector_length), representing the intermediate output vectors from a neural network.\n",
    "\n",
    "`y`: A 2D tensor of the same shape (batch_size, vector_length), where each row is a one-hot vector indicating the correct class.\n",
    "\n",
    "Output:\n",
    "\n",
    "`my_softmax(x)`: A 2D tensor of the same shape (batch_size, vector_length), where each row represents the probability distribution.\n",
    "\n",
    "`my_cross_entropy(x, y)`: A single float, representing the average cross-entropy loss over the batch.\n",
    "\n",
    "*Example*:\n",
    "\n",
    "Input:\n",
    "\n",
    "```python\n",
    "x = torch.Tensor([[2.0, 1.0, 0.1],\n",
    "                  [1.0, 3.0, 0.2]])\n",
    "\n",
    "y = torch.Tensor([[1, 0, 0],\n",
    "                  [0, 1, 0]])\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "my_softmax(x) = torch.Tensor([[0.659, 0.242, 0.099],\n",
    "                              [0.106, 0.873, 0.021]])\n",
    "\n",
    "my_cross_entropy(x, y) = 0.648 # approximately\n",
    "```\n",
    "**Hints**:\n",
    "\n",
    "`torch.exp(x):` Returns a new tensor with the exponential of the elements of the input tensor (shape unchanged).\n",
    "\n",
    "`torch.log(x):` Returns a new tensor with the natural logarithm of the elements of input (shape unchanged).\n",
    "\n",
    "`torch.sum(x, dim, keepdim):` Returns the sum of elements in the input tensor `x`. The `dim` argument specifies the dimension along which the sum will be computed. The `keepdim` argument, a boolean, determines whether the output tensor will retain the reduced dimension. If `keepdim=True`, the output tensor will have the same number of dimensions as the input, with the summed dimension being of size 1. If `keepdim=False` (the default), the reduced dimension is squeezed (i.e., removed) from the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3b848-d5c7-4bc1-ba06-c541c20be805",
   "metadata": {
    "id": "baf3b848-d5c7-4bc1-ba06-c541c20be805",
    "outputId": "b9d79445-979c-4d6c-adbc-9fe60c37be62"
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import datetime\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def my_softmax(x):\n",
    "    ##########################\n",
    "    # YOUR CODE STARTS HERE\n",
    "    exp_x = torch.exp(x)\n",
    "    result = exp_x/ torch.sum(exp_x, dim = 1, keepdim=True)\n",
    "    \n",
    "    # Compute softmax\n",
    "    return result\n",
    "    # YOUR CODE ENDS HERE\n",
    "    ##########################\n",
    "\n",
    "\n",
    "def my_cross_entropy(x, y):\n",
    "    probs = my_softmax(x)\n",
    "    ##########################\n",
    "    # YOUR CODE STARTS HERE\n",
    "    rows, columns = x.size()\n",
    "    results = 0\n",
    "    for i in range(rows):\n",
    "        correct_idx = (y[i].argmax().item())\n",
    "        results += -1*torch.log(probs[i, correct_idx])\n",
    "    return results/rows\n",
    "    # YOUR CODE ENDS HERE\n",
    "    ##########################\n",
    "\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea658f5b-d9a2-47f6-9ead-7fdd31653f0d",
   "metadata": {
    "id": "ea658f5b-d9a2-47f6-9ead-7fdd31653f0d",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "a5b44542-0808-4b34-e55d-4028f14e4305"
   },
   "outputs": [],
   "source": [
    "# You must run this cell, but you don't need to expand it if you'd prefer not to see the details.\n",
    "\n",
    "batch_size = 4\n",
    "vector_length = 5\n",
    "\n",
    "x = torch.randn(batch_size, vector_length)\n",
    "y = torch.zeros(batch_size, vector_length)\n",
    "y[torch.arange(batch_size), torch.randint(0, vector_length, (batch_size,))] = 1  # One-hot encoding\n",
    "\n",
    "torch_softmax = torch.softmax(x, dim=1)\n",
    "torch_cross_entropy = torch.nn.CrossEntropyLoss()(x, y.argmax(dim=1)).item()\n",
    "\n",
    "if torch.allclose(my_softmax(x), torch_softmax) and abs(my_cross_entropy(x, y) - torch_cross_entropy) < 1e-6:\n",
    "    print(\"Well Done!\")\n",
    "else:\n",
    "    print(\"Try again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5598e625-5c9b-4584-8031-b10b65b04259",
   "metadata": {
    "id": "5598e625-5c9b-4584-8031-b10b65b04259"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ztzJM_Or2m4o",
   "metadata": {
    "id": "ztzJM_Or2m4o"
   },
   "source": [
    "## Exercise 2 : Implement the 2D Max Pooling layer.\n",
    "Your task is to implement nn.MaxPool2d() function (Lecture05 lab02) without using using any built-in Pytorch MaxPool2d modules. For simplicity, the shape of input data is (width, height), where we do not consider batch size and channel size.\n",
    "\n",
    "Ensure that the final output is \"Well Done!\".\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "`torch.max(x):` Returns the maximal value (scalar number) of a tensor matrix x. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PQdhG5WU1POv",
   "metadata": {
    "id": "PQdhG5WU1POv",
    "outputId": "7884b5ca-41bf-49e1-ddd1-048accb82284"
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import datetime\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class MaxPool2d:\n",
    "    def __init__(self, pooling_size):\n",
    "        # pooling_size: an integer\n",
    "        self.pooling_size = pooling_size\n",
    "\n",
    "    def do_pooling(self, data):\n",
    "        # Shape of data: (width, height)\n",
    "        temp_data = data.numpy()\n",
    "        width, height = data.shape\n",
    "        ##########################\n",
    "        # YOUR CODE STARTS HERE\n",
    "        stride = self.pooling_size\n",
    "        output_size = width // self.pooling_size\n",
    "        output_matrix = np.zeros((output_size, output_size))\n",
    "\n",
    "        # Perform pooling\n",
    "        for i in range(width):\n",
    "            for j in range(height):\n",
    "                # Extract pooling region\n",
    "                region = data[i * stride: i * stride + self.pooling_size, j * stride: j * stride + self.pooling_size]\n",
    "                # print(region)\n",
    "                # Apply pooling operation\n",
    "                output_matrix[i, j] = np.max(region)\n",
    "\n",
    "        result = torch.Tensor(output_matrix)\n",
    "        # YOUR CODE ENDS HERE\n",
    "        ##########################\n",
    "        return result\n",
    "\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eHqnpeJB1PRJ",
   "metadata": {
    "id": "eHqnpeJB1PRJ",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "130b93c7-e01b-4406-f90f-5c9f2415b906"
   },
   "outputs": [],
   "source": [
    "# You must run this cell, but you don't need to expand it if you'd prefer not to see the details.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "rand_pool_size = torch.randint(5, 10, (1,)).item()\n",
    "sample_data = torch.rand(1, 1, 4*rand_pool_size, 5*rand_pool_size)\n",
    "mod = nn.MaxPool2d(rand_pool_size, rand_pool_size)\n",
    "test_mod = MaxPool2d(rand_pool_size)\n",
    "true_pool = mod(sample_data)[0,0]\n",
    "test_pool = test_mod.do_pooling(sample_data[0, 0])\n",
    "if (true_pool-test_pool).sum() < 1e-9:\n",
    "    print(\"Well Done!\")\n",
    "else:\n",
    "    print(\"Try again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dbdb40-49be-496d-bd71-0ec81c8e1d9c",
   "metadata": {
    "id": "44dbdb40-49be-496d-bd71-0ec81c8e1d9c"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "SeVmtkbB2p5I",
   "metadata": {
    "id": "SeVmtkbB2p5I"
   },
   "source": [
    "## Exercise 3 : Implement MLP with sigmoid activation.\n",
    "\n",
    "Implement a simple two-layer multi-layer perceptron (MLP) in PyTorch without using the pre-built modules `torch.nn.Linear()` or `torch.nn.Sigmoid()`.\n",
    "\n",
    "For the first hidden linear layer, use a weight tensor W1 with shape (in_features, hidden_size) and a bias tensor b1 with shape (hidden_size,).\n",
    "\n",
    "For the output linear layer, employ a weight tensor W2 with shape (hidden_size, out_features) and a bias tensor b2 with shape (out_features,).\n",
    "\n",
    "The activation after the hidden linear layer is the sigmoid function defined as: $\\textrm{sigmoid}(x) = 1 / (1 + \\exp(-x))$.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "`torch.exp(x):` Returns a new tensor with the exponential of the elements of the input tensor (shape unchanged).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CYcwVLEr2ref",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1740223456939,
     "user": {
      "displayName": "刘子铭",
      "userId": "07876562238380223127"
     },
     "user_tz": -480
    },
    "id": "CYcwVLEr2ref",
    "outputId": "e1022379-0418-40bf-9c4c-e37747983532"
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "class MyMLP:\n",
    "    \"\"\"\n",
    "    A simple MLP with one hidden layer using manual matrix multiplication\n",
    "    and manual sigmoid, without torch.nn.Linear or torch.sigmoid.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        # Weights and biases are randomly initialized with :\n",
    "        #  1) Hidden layer: (input_dim  -> hidden_dim)\n",
    "        #  2) Output layer: (hidden_dim -> output_dim)\n",
    "        self.W1 = torch.randn(input_dim, hidden_dim, requires_grad=False)\n",
    "        self.b1 = torch.randn(hidden_dim, requires_grad=False)\n",
    "        self.W2 = torch.randn(hidden_dim, output_dim, requires_grad=False)\n",
    "        self.b2 = torch.randn(output_dim, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through:\n",
    "            hidden = sigmoid(W1 * x + b1)\n",
    "            output = W2 * hidden + b2\n",
    "        \"\"\"\n",
    "        ##########################\n",
    "        # YOUR CODE STARTS HERE\n",
    "        print(self.W1.shape)\n",
    "        print(x.shape)\n",
    "        self.layer1_output = x*self.W1.T\n",
    "        self.layer2_output = self.layer1_output*self.W2.T\n",
    "        out = 1/(1+torch.exp((self.layer2_output*-1)))\n",
    "        # YOUR CODE ENDS HERE\n",
    "        ##########################\n",
    "        return out\n",
    "\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4650d31a-5f5c-4619-ac29-cd39eedad92a",
   "metadata": {
    "id": "4650d31a-5f5c-4619-ac29-cd39eedad92a",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "0eea38d2-1c71-4f70-9c56-163fce83263a"
   },
   "outputs": [],
   "source": [
    "# You must run this cell, but you don't need to expand it if you'd prefer not to see the details.\n",
    "\n",
    "class ReferenceMLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Reference MLP using official PyTorch modules for comparison.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, output_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.sigmoid(self.fc1(x))\n",
    "        return self.fc2(hidden)\n",
    "\n",
    "# We set the same dimensions\n",
    "input_dim, hidden_dim, output_dim = 5, 10, 2\n",
    "\n",
    "# Create random input data\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(4, input_dim)\n",
    "\n",
    "# Create our MLP\n",
    "my_mlp = MyMLP(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Create the reference MLP\n",
    "ref_mlp = ReferenceMLP(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Copy parameters from the reference MLP to our MLP for a fair comparison\n",
    "with torch.no_grad():\n",
    "    my_mlp.W1[:] = ref_mlp.fc1.weight.data.T\n",
    "    my_mlp.b1[:] = ref_mlp.fc1.bias.data\n",
    "    my_mlp.W2[:] = ref_mlp.fc2.weight.data.T\n",
    "    my_mlp.b2[:] = ref_mlp.fc2.bias.data\n",
    "\n",
    "# Get outputs\n",
    "my_output = my_mlp.forward(x)\n",
    "ref_output = ref_mlp(x)\n",
    "\n",
    "# Compare\n",
    "diff = (my_output - ref_output).abs().max().item()\n",
    "# print(\"Difference:\", diff)\n",
    "\n",
    "# Check if the difference is very small\n",
    "if diff < 1e-7:\n",
    "    print(\"Well Done!\")\n",
    "else:\n",
    "    print(\"Try again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa60a5a-1aad-4891-8fc3-cf6b8244a3bb",
   "metadata": {
    "id": "ffa60a5a-1aad-4891-8fc3-cf6b8244a3bb"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eShosBo4w8Ay",
   "metadata": {
    "id": "eShosBo4w8Ay"
   },
   "source": [
    "## Exercise 4 : Implement a training loop for a two-layer MLP.\n",
    "\n",
    "In this exercise, you will implement a training loop for a two-layer Multi-Layer Perceptron using built-in Pytorch functions. The model will be trained on a small part of the CIFAR-10 dataset, which consists of 2,500 training labeled images and 500 test labeled images across 10 classes. Your goal is to complete the missing parts of the code and ensure the training loop functions correctly.\n",
    "\n",
    "### Your tasks\n",
    "1.\tDataset Splitting:\n",
    "\t- The small CIFAR training dataset contains 2,500 samples.\n",
    "\t- You need to further split this into 2,000 samples for training and 500 samples for validation.\n",
    "    - The original test data of 500 samples is unchanged.\n",
    "3.\tModel Implementation:\n",
    "  - Implement a class for the two-Layer MLP model.\n",
    "  - The architecture should follow the following structure: Linear → ReLU → Linear.\n",
    "3.\tTraining Loop Completion:\n",
    "\t- Implement the training loop that optimizes the model.\n",
    "\n",
    "After training, the trained model must achieve below 65% error rate on the test set. Ensure that the final output is \"Well Done!\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qz-0r12RGcPJ",
   "metadata": {
    "id": "qz-0r12RGcPJ",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# You must run this cell, but you don't need to expand it if you'd prefer not to see the details.\n",
    "\n",
    "%reset -f\n",
    "import datetime\n",
    "\n",
    "# Prerequisites: Load Necessary Libraries and Functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def get_error(scores, labels):\n",
    "    bs=scores.size(0)\n",
    "    predicted_labels = scores.argmax(dim=1)\n",
    "    indicator = (predicted_labels == labels)\n",
    "    num_matches=indicator.sum()\n",
    "    return 1-num_matches.float()/bs\n",
    "\n",
    "def eval_on_test_set(data, label, batch_size=1, split='test'):\n",
    "    running_error = 0\n",
    "    num_batches = 0\n",
    "    for i in range(0, data.shape[0],batch_size):\n",
    "        minibatch_data =  data[i:i+batch_size]\n",
    "        minibatch_label = label[i:i+batch_size]\n",
    "        inputs = minibatch_data.view(batch_size, torch.flatten(data, start_dim=1).shape[1])\n",
    "        scores=net(inputs)\n",
    "        error = get_error(scores, minibatch_label)\n",
    "        running_error += error.item()\n",
    "        num_batches+=1\n",
    "    total_error = running_error/num_batches\n",
    "    print( f'{split} error  = ', total_error*100 ,'percent')\n",
    "    return total_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rZpk0mfhViTT",
   "metadata": {
    "id": "rZpk0mfhViTT"
   },
   "source": [
    "### Data Splitting\n",
    "\n",
    "We split the original training dataset (2,500 samples) into:\n",
    "- Training set: 2,000 samples used for model learning.\n",
    "- Validation set: 500 samples used for learning rate tuning (and reducing overfitting).\n",
    "\n",
    "The original test 500 samples remain unchanged for final model evaluation.\n",
    "\n",
    "After performing the split, the expected output should match the following sizes :\n",
    "```\n",
    "train_data: torch.Size([2000, 3, 32, 32])\n",
    "train_label: torch.Size([2000])\n",
    "val_data: torch.Size([500, 3, 32, 32])\n",
    "val_label: torch.Size([500])\n",
    "test_data: torch.Size([500, 3, 32, 32])\n",
    "test_label: torch.Size([500])\n",
    "```\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "`torch.randperm(N):` Returns a random permutation of integers from 0 to N - 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WDV2UmVPDh5p",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33598,
     "status": "ok",
     "timestamp": 1740412403774,
     "user": {
      "displayName": "Zehong Yan",
      "userId": "08549061813386001319"
     },
     "user_tz": -480
    },
    "id": "WDV2UmVPDh5p",
    "outputId": "f26255f1-051f-4381-a77d-c4cc1d430842"
   },
   "outputs": [],
   "source": [
    "# Load the CIFAR data\n",
    "with open('datasets/small_CIFAR.pkl',\"rb\") as f:\n",
    "    train_data, train_label, test_data, test_label = pickle.load(f)\n",
    "\n",
    "# Define split sizes\n",
    "train_size = 2000\n",
    "val_size = 500\n",
    "\n",
    "##########################\n",
    "# YOUR CODE STARTS HERE\n",
    "# Generate random permutation of 2,500 indices\n",
    "shuffled_indices = torch.randperm(train_size+val_size)\n",
    "# Select the first 2,000 indices for training and the remaining 500 for evaluation\n",
    "train_indices = shuffled_indices[:2000]\n",
    "val_indices =  shuffled_indices[2000:]\n",
    "# Use the indices to create the training and validation sets\n",
    "val_data = train_data[val_indices]\n",
    "val_label = train_label[val_indices]\n",
    "train_data = train_data[train_indices]\n",
    "train_label = train_label[train_indices]\n",
    "# YOUR CODE ENDS HERE\n",
    "##########################\n",
    "\n",
    "print(f\"train_data: {train_data.size()}\")\n",
    "print(f\"train_label: {train_label.size()}\")\n",
    "print(f\"val_data: {val_data.size()}\")\n",
    "print(f\"val_label: {val_label.size()}\")\n",
    "print(f\"test_data: {test_data.size()}\")\n",
    "print(f\"test_label: {test_label.size()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25f1fe-68cd-4d3a-a4f0-2dadaddb1d3d",
   "metadata": {
    "id": "9e25f1fe-68cd-4d3a-a4f0-2dadaddb1d3d"
   },
   "source": [
    "### Model Implementation\n",
    "\n",
    "Implement a two-Layer MLP model with the class `two_layer_net()`.\n",
    "\n",
    "The architecture should follow the structure: Linear → ReLU → Linear.\n",
    "\n",
    "**Hints**: You may use modules `nn.Linear()` and `nn.ReLU()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mswpfoqxbWrj",
   "metadata": {
    "id": "mswpfoqxbWrj"
   },
   "outputs": [],
   "source": [
    "class two_layer_net(nn.Module):\n",
    "    \"\"\"\n",
    "    A two-layer fully-connected neural network with ReLU nonlinearity.\n",
    "    We assume an input dimension of 'input_size', a hidden dimension of 'hidden_size',\n",
    "    and perform classification over C classes ('output_size').\n",
    "    The architecure should be linear layer - relu layer - linear layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        # Define layers explicitly using pytorch modules\n",
    "        ##########################\n",
    "        # YOUR CODE STARTS HERE\n",
    "        self.layer1 = nn.Linear( input_size   , hidden_size  , bias=True)\n",
    "        self.layer2 = nn.Linear(  hidden_size , output_size  , bias=True)\n",
    "        # YOUR CODE ENDS HERE\n",
    "        ##########################\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##########################\n",
    "        # YOUR CODE STARTS HERE\n",
    "        x = self.layer1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        # YOUR CODE ENDS HERE\n",
    "        ##########################\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fX6IZo8WQIy",
   "metadata": {
    "id": "9fX6IZo8WQIy"
   },
   "source": [
    "### Implement the training loop that optimizes the model's parameters.\n",
    "\n",
    "Code the training loop:\n",
    "\n",
    "1.\tShuffle the training set at each epoch.\n",
    "  \n",
    "1.\tProcess the mini-batches of data:\n",
    "\n",
    "\t- Forward pass: Computes model prediction and loss value.\n",
    "    - Backward pass: Computes gradients of loss with respect to parameters.\n",
    "\t- Updates the parameters using SGD optimizer.\n",
    "    - Track training error.\n",
    "&nbsp;\n",
    "1.\tTrack validation error. If validation error does not decrease, then reduce the learning rate by a factor of 1.2.\n",
    "\n",
    "After training, the trained model must achieve beyond 65% error rate on the test set.\n",
    "\n",
    "You are free to select the initial learning rate, the batch size, the number of epochs, or anything else that is useful to achieve the target test error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441FNb7yNGgb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18699,
     "status": "ok",
     "timestamp": 1740412727256,
     "user": {
      "displayName": "Zehong Yan",
      "userId": "08549061813386001319"
     },
     "user_tz": -480
    },
    "id": "441FNb7yNGgb",
    "outputId": "d521d1b5-d67e-4be4-da72-ed8b2d3bfa9c"
   },
   "outputs": [],
   "source": [
    "# Initialize the two-layer neural network\n",
    "reshaped_input_size = torch.flatten(train_data, start_dim=1).shape[1] # calculate the input size, equal to 3*32*32 = 3072\n",
    "net = two_layer_net(reshaped_input_size, 100, 10)\n",
    "print('net: ',net)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set batch size for mini-batch gradient descent\n",
    "bs = 100\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Set initial learning rate\n",
    "lr = 1e-1\n",
    "\n",
    "loss_history = []\n",
    "train_error_history = []\n",
    "val_error_history = []\n",
    "test_error_history = []\n",
    "val_error_best = 100\n",
    "\n",
    "# create a new optimizer at the beginning of each epoch: give the current learning rate.\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    running_loss = 0\n",
    "    running_error = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    ##########################\n",
    "    # YOUR CODE STARTS HERE\n",
    "    shuffled_indices = torch.randperm(train_data.size(0))\n",
    "    # YOUR CODE ENDS HERE\n",
    "    ##########################\n",
    "\n",
    "    for count in range(0, train_data.shape[0], bs):\n",
    "\n",
    "        ##########################\n",
    "        # YOUR CODE STARTS HERE\n",
    "\n",
    "        # Zero out gradients to avoid accumulation from previous iterations\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Select a mini-batch of training data and corresponding labels\n",
    "        indices = shuffled_indices[count: count+bs]\n",
    "        minibatch_data = train_data[indices]\n",
    "        minibatch_label = train_label[indices]\n",
    "\n",
    "        # Reshape input data to match the expected input format of the network\n",
    "        inputs = minibatch_data.view(bs, 3*32*32)\n",
    "\n",
    "        # Enable gradient tracking for inputs\n",
    "        inputs.requires_grad_()\n",
    "\n",
    "        # Forward pass: compute predictions (scores) using the neural network\n",
    "        scores = net(inputs)\n",
    "        # Compute loss using the defined loss function\n",
    "        loss = criterion(scores, minibatch_label)\n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update network parameters based on gradients\n",
    "        optimizer.step()\n",
    "        # Accumulate loss (detached to prevent computation graph issues)\n",
    "        running_loss += loss.detach().item()\n",
    "        # Compute error rate for the current batch\n",
    "        error = get_error(scores.detach(), minibatch_label)\n",
    "\n",
    "        # YOUR CODE ENDS HERE\n",
    "        ##########################\n",
    "\n",
    "        # Store loss history for later visualization\n",
    "        loss_history.append(loss.detach().item())\n",
    "\n",
    "        # Increment batch counter\n",
    "        num_batches += 1\n",
    "\n",
    "    # Once the epoch is finished we divide the \"running quantities\"\n",
    "    # by the number of batches to get the mean quantities over an epoch\n",
    "    total_loss = running_loss/ num_batches\n",
    "    total_error = running_error/ num_batches\n",
    "    elapsed_time = time.time() - start\n",
    "\n",
    "    # Compute the error rate on the val set and test set\n",
    "    val_error = eval_on_test_set(val_data, val_label, batch_size=bs, split='val')\n",
    "    test_error = eval_on_test_set(test_data, test_label, batch_size=bs, split='test')\n",
    "\n",
    "    # If validation error does not decrease, then reduce the learning rate by a factor\n",
    "    if val_error < val_error_best:\n",
    "        val_error_best = val_error\n",
    "    else:\n",
    "        ##########################\n",
    "        # YOUR CODE STARTS HERE\n",
    "        lr = lr/1.2\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "        # YOUR CODE ENDS HERE\n",
    "        ##########################\n",
    "\n",
    "    # Display stats for every epoch and store new quantities\n",
    "    print('epoch=',epoch, ' time=', elapsed_time,\n",
    "          ' loss=', total_loss , ' error=', total_error*100 ,'percent lr=', lr)\n",
    "    print('-'*100)\n",
    "    train_error_history.append(error)\n",
    "    val_error_history.append(val_error)\n",
    "    test_error_history.append(test_error)\n",
    "\n",
    "\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "# Visualize train / val / test error\n",
    "plt.figure()\n",
    "plt.title('Error Rate')\n",
    "plt.plot(train_error_history, '-o', label='train')\n",
    "plt.plot(val_error_history, '-o', label='val')\n",
    "plt.plot(test_error_history, '-o', label='test')\n",
    "plt.plot([0.65] * len(val_error_history), 'k--', label='threshold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Check if the difference is very small\n",
    "if min(test_error_history) < 0.65:\n",
    "    print(\"Well Done!\")\n",
    "else:\n",
    "    print(\"Try again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vDqdbzYW2uf4",
   "metadata": {
    "id": "vDqdbzYW2uf4"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "WSIFDnr9kkcK",
   "metadata": {
    "id": "WSIFDnr9kkcK"
   },
   "source": [
    "## Exercise 5 : Implement the 2D convolutional layer.\n",
    "\n",
    "The exercise's goal is to implement the 2D convolution function without using any built-in pytorch convolution functions.\n",
    "\n",
    "For simplicity, we assume that there is no padding and the stride value is 1.\n",
    "\n",
    "Ensure that the final output is \"Well Done!\".\n",
    "\n",
    "*Inputs*:\n",
    "\n",
    "`x`: A 4D tensor of shape (batch_size, num_channel, image_height, image_width), representing a batch of input images.\n",
    "\n",
    "`K`: A 4D tensor of shape (num_kernel, num_channel, kernel_height, kernel_width) representing the kernels to process the image.\n",
    "\n",
    "*Outputs*:\n",
    "\n",
    "`my_conv2d(x, K)`: A 4D tensor of the same shape (batch_size, num_kernel, image_height, image_width).\n",
    "\n",
    "**Grading**:\n",
    "\n",
    "Using more than 4 \"for loops\" will not give you any marks.\n",
    "\n",
    "Using 4 \"for loops\" will give you 33% of the marks.\n",
    "\n",
    "Using 3 \"for loops\" will give you 66% of the marks.\n",
    "\n",
    "Using 2 \"for loops\" will give you the full 100%.\n",
    "\n",
    "**Useful Functions**:\n",
    "\n",
    "`x.size():` Returns the size of the tensor.\n",
    "\n",
    "`torch.zeros(*size): ` Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.\n",
    "\n",
    "`torch.flatten(input, start_dim=0, end_dim=-1):` Flattens input by reshaping it into a one-dimensional tensor. If start_dim or end_dim are passed, only dimensions starting with start_dim and ending with end_dim are flattened. The order of elements in input is unchanged.\n",
    "\n",
    "`torch.unsqueeze(input, dim): `Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "Pytorch supports *broadcasting* which is useful for vectorized operations, for example:\n",
    "```python\n",
    "x = torch.Tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]]) # Shape of (3, 3)\n",
    "y = torch.Tensor([[2, 2, 2]]) # Shape of (1, 3)\n",
    "\n",
    "print(x*y) # Returns [[2, 4, 6], [8, 10, 12], [14, 16, 18]]\n",
    "```\n",
    "\n",
    "The y tensor matches the x tensor on the second dimension (both values are 3). However, the y tensor does not match the x tensor on the first dimension (1 vs. 3). Pytorch automatically repeats the y tensor across the first dimension to match the shape of x. More generally, pytorch can broadcast two tensors if for each dimension, either the value matches or the value of one of the tensors is 1. For example, pytorch can broadcast:\n",
    "```python\n",
    "x = torch.rand(1, 50, 1, 50, 50, 30)\n",
    "y = torch.rand(30, 1, 50, 1, 50, 30)\n",
    "\n",
    "print(x*y) # Size of (30, 50, 50, 50, 50, 30)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_Rl0KYgqix88",
   "metadata": {
    "id": "_Rl0KYgqix88"
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "def my_conv2d(x, K):\n",
    "    ##########################\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # Get dimensions\n",
    "    input_size = x.shape[0]\n",
    "    kernel_size = K.shape[0]\n",
    "\n",
    "    # Pad the input matrix\n",
    "    padded_input = np.pad(x, pad_width=padding, mode='constant', constant_values=0)\n",
    "\n",
    "    # Compute output dimensions\n",
    "    output_size = ((input_size - kernel_size + 2 * padding) // stride) + 1\n",
    "\n",
    "    # Initialize output matrix\n",
    "    output_matrix = np.zeros((output_size, output_size))\n",
    "\n",
    "    # Perform convolution\n",
    "    for i in range(0, output_size):\n",
    "        for j in range(0, output_size):\n",
    "            # Extract region of input\n",
    "            region = padded_input[i * stride:i * stride + kernel_size, j * stride:j * stride + kernel_size]\n",
    "            # Apply convolution (element-wise multiplication and sum)\n",
    "            output_matrix[i, j] = np.sum(region * k)\n",
    "\n",
    "    return output_matrix\n",
    "    return out\n",
    "    # YOUR CODE ENDS HERE\n",
    "    ##########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AZk90ygX2yw5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 88,
     "status": "ok",
     "timestamp": 1740314464930,
     "user": {
      "displayName": "Chew Kin whye",
      "userId": "16899899049406240390"
     },
     "user_tz": -480
    },
    "id": "AZk90ygX2yw5",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "07ec63ec-62f2-4b7c-f479-140609aa8aab"
   },
   "outputs": [],
   "source": [
    "# You must run this cell, but you don't need to expand it if you'd prefer not to see the details.\n",
    "\n",
    "import torch\n",
    "\n",
    "x1 = torch.randint(100, (4, 3, 28, 28)) # Batch of input images\n",
    "K1 = torch.randint(100, (8, 3, 3, 3))\n",
    "\n",
    "x2 = torch.randint(100, (4, 3, 32, 32)) # Batch of input images\n",
    "K2 = torch.randint(100, (8, 3, 5, 5))\n",
    "\n",
    "print('Timestamp:',datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\"))\n",
    "\n",
    "if (my_conv2d(x1, K1) - torch.nn.functional.conv2d(x1, K1, bias=None, stride=1, padding=0)).abs().max().item() + (my_conv2d(x2, K2) - torch.nn.functional.conv2d(x2, K2, bias=None, stride=1, padding=0)).abs().max().item()< 1e-6:\n",
    "    print(\"Well Done!\")\n",
    "else:\n",
    "    print(\"Try again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566962a1-2fa9-4a8c-bc90-919d545ee639",
   "metadata": {
    "id": "566962a1-2fa9-4a8c-bc90-919d545ee639"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3421c4b-4ad6-4dfa-a629-2baf5e76dd63",
   "metadata": {
    "id": "d3421c4b-4ad6-4dfa-a629-2baf5e76dd63"
   },
   "source": [
    "## End of coding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6dfadd-e1cc-4578-abd7-5945bfa3eec2",
   "metadata": {
    "id": "ce6dfadd-e1cc-4578-abd7-5945bfa3eec2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deeplearn_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
